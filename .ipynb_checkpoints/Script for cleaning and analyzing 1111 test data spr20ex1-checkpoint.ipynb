{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cleans .txt and .dta data for students performance on exams for BIO1111 and exports a csv which can be analyzed in an R script.\n",
    "#### Author: Christopher Agard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,glob\n",
    "from soar import *\n",
    "\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting source and output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu={'worksource': \"C:/Users/tuh27554/Documents/Grading/2020/SPR EX1/Input Data\",\n",
    "    'workoutput': \"C:/Users/tuh27554/Documents/Grading/2020/SPR EX1/Output Data\"}\n",
    "gandolf={'worksource': \"C:/Users/craga/Google Drive/TU 2020/SPR/Exam 1/Input data\",\n",
    "    'workoutput': \"C:/Users/craga/Google Drive/TU 2020/SPR/Exam 1/Output Data\"}\n",
    "\n",
    "source = tu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and Cleaning Exam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(source['worksource'])\n",
    "files = glob.glob('*.dta')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(source['worksource'])\n",
    "df1=cleanSoarExam(files[0],examNum=1,soarSessions=[81])\n",
    "df1['version'] = 1\n",
    "df2=cleanSoarExam(files[1],examNum=1,soarSessions=[81])\n",
    "df2['version'] = 2\n",
    "df = df1.append(df2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will confirm that the exam key have been appropriately identified and change it if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we change the dir to the outputfolder and save the cleaned, merged file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemcols = df.columns[df.columns.str.contains('item')]\n",
    "itemcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in itemcols:\n",
    "    df[col] = df[col].apply(letters2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(source['workoutput'])\n",
    "df.to_csv(\"Fall19Exam 1_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Exam Data\n",
    "\n",
    "1. [Most Frequently Wrong](#Most-Frequently-Wrong-Answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strange values for items\n",
    "\n",
    "In reviewing the data I noted some values outside of the expected 1-4.  Could you help me understand what these are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses = pd.concat([pd.Series(df[col].unique()) for col in itemcols]).unique()\n",
    "strange_response = [resp for resp in all_responses if resp not in ['A','B','C','D','E','1','2','3','4','5']]\n",
    "strange_resp_item=[col for col in itemcols if df.loc[df[col].isin(strange_response),col].shape[0]>0]\n",
    "print(\"The strange responses {} were found for the following items:\\n{}\".format(strange_response, strange_resp_item))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Frequently Wrong Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we identify the columns containing information about responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.soarType.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply this function to create a ranked list of items by order of the number of my students getting them wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrongcount_v1_mine = pd.DataFrame()\n",
    "for col in itemcols:\n",
    "    wrongcount_v1_mine = wrongcount_v1_mine.append(countwrong(df= df.loc[df.soar==81], col = col, ver = 1,compind=['mine','other']))\n",
    "wrongcount_v1_mine = wrongcount_v1_mine.sort_values(['number','item'],ascending=False).reset_index(drop = True)\n",
    "# wrongcount_v1_mine.index = wrongcount_v1_mine.item\n",
    "# wrongcount_v1_mine= wrongcount_v1_mine.drop(columns = 'item')\n",
    "wrongcount_v1_mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrongcount_v2_mine = pd.DataFrame()\n",
    "for col in itemcols:\n",
    "    wrongcount_v2_mine = wrongcount_v2_mine.append(countwrong(df= df.loc[df.soar==81], col = col, ver = 2,compind=['mine','other']))\n",
    "wrongcount_v2_mine = wrongcount_v2_mine.sort_values(['number','item'],ascending=False).reset_index(drop = True)\n",
    "wrongcount_v2_mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_wrong81 = wrongcount_v1_mine.merge(wrongcount_v2_mine,how='left',on= 'item').set_index(['item']).sum(axis=1)\n",
    "total_wrong81.to_csv('total wrong 81.csv')\n",
    "total_wrong81.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "v1keys = df.loc[(df.soar.isnull())&(df.soarType=='key')&(df.version==1),itemcols].iloc[0]\n",
    "v2keys = df.loc[(df.soarType=='key')&(df.version==2),itemcols].iloc[0]\n",
    "keys = pd.DataFrame(data = {'version1':v1keys,'version2':v2keys})\n",
    "keys.to_csv('1111 exam 1 S2019 keys.csv',index = False)\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Student Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score distribution\n",
    "Now we look at scores by section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ncorrect.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['soar']).ncorrect.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Analysis\n",
    "1. [Percent choice for each item](#Percent-choice-for-each-item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(source['worksource'])\n",
    "df18 = pd.read_csv('Fall 2018v1.csv')\n",
    "dfcomp = pd.read_csv('Cross-Walk 2019 Fall to 2018 Fall Exams.csv')\n",
    "dfcomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percent choice for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item1 = pd.DataFrame()\n",
    "for col in itemcols:\n",
    "    df_item1 = df_item1.append(df.loc[df.version==1,col].value_counts(normalize=True))\n",
    "df_item1 = df_item1.applymap(lambda x: \"{:.0f}\".format(x*100))\n",
    "df_item1 = df_item1.reset_index().rename(columns={'index':'F19E1'})\n",
    "df_item1['F19E1'] = df_item1.F19E1.apply(lambda x: int(x[5:]))\n",
    "df_item1 = df_item1.sort_values('F19E1')\n",
    "df_item1['F18E1'] = dfcomp.F18E1\n",
    "df_item1.to_csv('Fall 2019v1.csv')\n",
    "df_item1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_item = pd.DataFrame()\n",
    "for col in itemcols:\n",
    "    df_item = df_item.append(df.loc[df.version==2,col].value_counts(normalize=True))\n",
    "df_item = df_item.applymap(lambda x: \"{:.0f}\".format(x*100))\n",
    "df_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcomp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cleans .txt data for students performance on exams for BIO1111 and exports a csv which can be analyzed in an R script.\n",
    "#### Author: Christopher Agard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,glob\n",
    "\n",
    "def cleanSoarExam (data, examNum, fileType='flat',\n",
    "                   colSpec = [(0,9),(10,21),(22,28),(29,30),(30,32),(32,34),(34,36),(36,38),(39,41),(41,68)],\n",
    "                   soarSessions=[]):\n",
    "\n",
    "    if fileType != 'flat':\n",
    "        print(\"\\n{} fileType is not currently supported.\".format(fileType))\n",
    "    else:\n",
    "        try:\n",
    "            df=pd.read_fwf(data,colSpec,names=['tuid','last','first','middle','unnamed1','unnamed2','unnamed3','soar','ncorrect','item'])\n",
    "        except:\n",
    "            \"print(\\nCould not find file {} or {} was not acceptable value for colSpec)\".format(data,colSpec)\n",
    "        #try:\n",
    "        #    df.columns = ['tuid','last','first','middle','unnamed1','unnamed2','unnamed3','soar','ncorrect','item']\n",
    "        #except:\n",
    "        #    print(\"\\nColumn number != 10.\\n{}\".format(len(df.columns)))\n",
    "        try:\n",
    "            df['examNumber']=examNum\n",
    "            numbers=pd.Series(list(range(28))).astype(str)\n",
    "            itemNames= 'item_'+ numbers[1:]\n",
    "            itemData=df.item.apply(lambda i: pd.Series(list(i)))\n",
    "            itemData.columns=itemNames\n",
    "            df=df.merge(itemData,'outer',left_index=True,right_index=True).drop('item',axis=1)\n",
    "        except:\n",
    "            print(\"\\nUnhandled exception encountered.\")\n",
    "        if soarSessions ==np.nan:\n",
    "            df.loc['soarType']='other'\n",
    "        else:\n",
    "            df.loc[df.soar.isin(soarSessions),'soarType']='mine'\n",
    "            df.loc[~df.soar.isin(soarSessions),'soarType']='other'\n",
    "            df.loc[df.tuid=='NNNNNNNNN','soarType']='key'\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we need to write a function to determine how many students get each item wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "def nwrong (x,key):\n",
    "    \"\"\"\n",
    "    :param x: pd.Series\n",
    "    :param key: ~None\n",
    "    Takes a pandas series and a specified value and returns the number of values \n",
    "    in the series which do not match the specified value.\"\"\"\n",
    "    assert isinstance(x, pd.Series)\n",
    "    x = x.astype(str)\n",
    "    key = str(key)\n",
    "    return x[x!=key].count()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(string.ascii_lowercase)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,glob\n",
    "\n",
    "pd.options.display.max_columns=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Exam Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define paths for getting exam data and outputting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "homesource = 'S:/Chris/Temple/Biol1111/Fall 2018/Raw Data/Exam 2'\n",
    "homeoutputFolder = 'S:/Chris/Temple/Biol1111/Fall 2018/Results/Exam 2'\n",
    "worksource = 'C:/Users/tuh27554/Documents/BIOL1111/Fall 2018/Raw Data/Exam 2'\n",
    "workoutputFolder = 'C:/Users/tuh27554/Documents/BIOL1111/Fall 2018/Results/Exam 2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a list of source data files. For this notebook we will use the \\*.txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/tuh27554/Documents/BIOL1111/Fall 2018/Raw Data/Exam 2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b5c161735fd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworksource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'*.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/tuh27554/Documents/BIOL1111/Fall 2018/Raw Data/Exam 2'"
     ]
    }
   ],
   "source": [
    "os.chdir(homesource)\n",
    "files = glob.glob('*.txt')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " And now we read in and clean those data using the *cleanSoarExam* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = cleanSoarExam(files[0],examNum=1,soarSessions=[81])\n",
    "print('Version 1 of the exam has {} students.'.format(df1.shape[0]-1))\n",
    "df2 = cleanSoarExam(files[1],examNum=1,soarSessions=[81])\n",
    "print('Version 2 of the exam has {} students.'.format(df2.shape[0]-1))\n",
    "os.chdir(workoutputFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df1.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df2.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Exam Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the most problematic items for the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to identify the item columns over which to apply *nwrong*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemcols1 = df1.columns[df1.columns.str.contains('item_')]\n",
    "itemcols2 = df2.columns[df2.columns.str.contains('item_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the function to determine the number of students who answered incorrectly for version 1 and version 2 separately. We will only print one of these here for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1wrong = df1.loc[df1.soarType.isin(['mine','key']),itemcols1].apply(lambda x: nwrong(x=x[1:],key=x[0]))\n",
    "v2wrong = df2.loc[df2.soarType.isin(['mine','key']),itemcols2].apply(lambda x: nwrong(x=x[1:],key=x[0]))\n",
    "v2wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding these lists together we get the total number wrong on each item.  If we sort the resulting series in descending order, we will have the guide we need to determine the order for discussing the items in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalwrong = v1wrong + v2wrong\n",
    "orderedwrong = totalwrong.sort_values(ascending=False)\n",
    "orderedwrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also print the letters for the correct answers for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v1keys = df1.loc[df1.soarType=='key',itemcols1].apply(lambda x: list(string.ascii_uppercase)[int(x)-1])\n",
    "v1keys\n",
    "# v2wrong = df2.loc[df2.soarType.isin(['mine','key']),itemcols2].apply(lambda x: nwrong(x=x[1:],key=x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we need to write a function to determine how many students get each item wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2keys = df1.loc[df1.soarType=='key',itemcols2].apply(lambda x: list(string.ascii_uppercase)[int(x)-1])\n",
    "v2keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating students performace "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at scores by section.  For this we can rename *unnamed1* to *version* to keep track of the versions and append the 2 dfs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.append(df2)\n",
    "print(df.shape)\n",
    "df.to_csv('merged exam 2 results.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the \"key\" data and store as a separate variable,*maxscore*, the maximum possible value for ncorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxscore = df.ncorrect.max()\n",
    "df = df.loc[df.soarType!='key',:]\n",
    "print('maxscore:{}'.format(maxscore))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this *df* for the rest of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = df.groupby(['soar']).ncorrect.agg(['min','max','median','mean','count']).sort_values('median')\n",
    "scores.to_csv('Exam1Score breakdown.csv')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('soar').ncorrect.quantile(.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the distribution of individuals who scored above the 75-percentile in their class.  This gives us an idea of how distributed the high scores are in each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highAchieverslocal = df.groupby('soar').ncorrect.apply(lambda x: x.loc[x>x.quantile(.75)].count()).reset_index()\\\n",
    ".rename(columns = {'ncorrect':'nAbove75p'})\n",
    "highAchieverslocal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the distribution of individuals who scored above the 75-percentile across the entire class.  This gives us an idea of how distributed the high scores are across sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highAchieversGlobal = df.loc[df.ncorrect>df.ncorrect.quantile(.75)]\\\n",
    ".groupby('soar').ncorrect.count().reset_index()\\\n",
    ".rename(columns = {'ncorrect':'nAbove75p'})\n",
    "print('The overall all median and 75 percentile were {} and {}, \\\n",
    "respectively.'.format(df.ncorrect.median(),df.ncorrect.quantile(.75)))\n",
    "highAchieversGlobal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
